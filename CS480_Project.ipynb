{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet \"xlrd\" \"scikit-learn\" \"ipython[notebook]==7.34.0, <8.17.0\" \"openpyxl\" \"fastparquet\" \"lightgbm\" \"pyarrow\" \"setuptools>=68.0.0, <68.3.0\"  \"xgboost\" \"catboost\" \"tensorboard\" \"lightning>=2.0.0\" \"urllib3\" \"torch==2.3.0\" \"matplotlib\" \"optuna\" \"pytorch-lightning>=1.4, <2.1.0\" \"seaborn\" \"torchvision\" \"torchmetrics>=0.7, <1.3\" \"matplotlib>=3.0.0, <3.9.0\" \"numpy==1.26.4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint, EarlyStopping\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "TRAIN_IMAGES_DATA_PATH = \"./train_images\"\n",
    "TEST_IMAGES_DATA_PATH = \"./test_images\"\n",
    "CHECKPOINT_PATH = os.environ.get(\"PATH_CHECKPOINT\", \"saved_models/\")\n",
    "\n",
    "plt.set_cmap(\"cividis\")\n",
    "%matplotlib inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"png\", \"svg\", \"pdf\")  # For export\n",
    "sns.reset_orig()\n",
    "\n",
    "L.seed_everything(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from data_set import PlantDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "img_transform_train = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "img_transform_test = test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(size=(224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# do not normalize CSV features until split to avoid data leakage\n",
    "partial_training_data = PlantDataset('train.csv', 'train_images', num_labels=6, image_transform=img_transform_train)\n",
    "partial_test_data = PlantDataset('test.csv', 'test_images', num_labels=0, image_transform=img_transform_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_set import AugmentedDataset\n",
    "\n",
    "# Augment data sets\n",
    "\n",
    "## Change this when regenerating embeddings\n",
    "model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14_reg').to(device)\n",
    "# model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitg14_reg').to(device)\n",
    "full_training_data = AugmentedDataset(partial_training_data, model, \"train_embeddings.parquet\", device=device)\n",
    "test_data = AugmentedDataset(partial_test_data, model, \"test_embeddings.parquet\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_set import PandasDataset\n",
    "\n",
    "class MLPModel(L.LightningModule):\n",
    "    def __init__(self, input_dim=1699, output_dim=6, lr=5e-4, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(256, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, row):\n",
    "        x = self.body(row)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=self.hparams.lr, weight_decay=0.0005)\n",
    "        return [optimizer], []\n",
    "\n",
    "    def _calculate_loss(self, batch, mode=\"train\"):\n",
    "        rows, labels = batch\n",
    "\n",
    "        preds = self.forward(rows)\n",
    "        loss = F.mse_loss(torch.squeeze(preds), torch.squeeze(labels))\n",
    "        self.log(f\"{mode}_loss\", loss)\n",
    "        self.log(f\"{mode}_r2\", r2_score(labels.cpu().numpy(), preds.detach().cpu().numpy()))\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode=\"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self._calculate_loss(batch, mode=\"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X : pd.DataFrame):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(torch.tensor(X.values, dtype=torch.float32).to(device)).cpu().numpy()\n",
    "    \n",
    "    def score(self, X : pd.DataFrame, Y: pd.DataFrame):\n",
    "        return r2_score(Y, self.predict(X))\n",
    "\n",
    "class MyDataModule(L.LightningDataModule):\n",
    "    def __init__(self, X_train, Y_train, X_val, Y_val, batch_size=512):\n",
    "        super().__init__()\n",
    "        self.mlp_train_set = PandasDataset(X_train, Y_train)\n",
    "        self.mlp_val_set = PandasDataset(X_val, Y_val)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mlp_train_set, batch_size=self.batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mlp_val_set, batch_size=self.batch_size, num_workers=2)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "import os\n",
    "def train_mlp_model(X_train, Y_train, X_val, Y_val, batch_size=256, dry_run=False, run_num=0, **kwargs):\n",
    "    mlp_data_module = MyDataModule(X_train, Y_train, X_val, Y_val, batch_size=batch_size)\n",
    "    trainer = L.Trainer(\n",
    "        default_root_dir=os.path.join(CHECKPOINT_PATH, f\"{run_num}/mlp/\"),\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        max_epochs=5 if not dry_run else 1,\n",
    "        callbacks=[\n",
    "            ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"epoch\"),\n",
    "            EarlyStopping(monitor='val_r2', patience=1, mode=\"max\"),\n",
    "            LearningRateMonitor(\"epoch\"),\n",
    "        ],\n",
    "        enable_progress_bar=False\n",
    "    )\n",
    "    trainer.logger._log_graph = True\n",
    "    trainer.logger._default_hp_metric = None\n",
    "\n",
    "    model = MLPModel(\n",
    "        input_dim=X_train.shape[1],\n",
    "        output_dim=Y_val.shape[1] if not dry_run else 1,\n",
    "        **kwargs\n",
    "    )\n",
    "    trainer.fit(model, datamodule=mlp_data_module)\n",
    "\n",
    "    # Load the best checkpoint after training\n",
    "    model = MLPModel.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens tensorboard in notebook. Adjust the path to your CHECKPOINT_PATH!\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ./saved_models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, ShuffleSplit\n",
    "import xgboost\n",
    "import catboost\n",
    "import lightgbm\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from plant_trait_regressor import PlantTraitRegressor\n",
    "\n",
    "# do a quick run through\n",
    "dry_run = False\n",
    "\n",
    "kf = ShuffleSplit(\n",
    "    n_splits=5 if not dry_run else 1, \n",
    "    random_state=42, \n",
    "    test_size=0.1\n",
    ")\n",
    "preds_test = np.zeros((len(test_data), 6 if not dry_run else 1))\n",
    "\n",
    "if dry_run:\n",
    "    print (\"Dry running code...\")\n",
    "\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(kf.split(full_training_data.csv_aug, full_training_data.labels)):\n",
    "    # train sets\n",
    "    X_train = full_training_data.csv_aug.iloc[train_index].copy()\n",
    "    Y_train = full_training_data.labels.iloc[train_index].copy()\n",
    "    \n",
    "    # validation sets\n",
    "    X_val = full_training_data.csv_aug.iloc[test_index].copy()\n",
    "    Y_val = full_training_data.labels.iloc[test_index].copy()\n",
    "    \n",
    "    if dry_run:\n",
    "        Y_train = pd.DataFrame(Y_train.iloc[:, 0])\n",
    "        Y_val = pd.DataFrame(Y_val.iloc[:, 0])\n",
    "    \n",
    "    # Test set\n",
    "    X_test = test_data.csv_aug.copy()\n",
    "    \n",
    "    # for boosting algorithms, it can be beneficial to engineer some features\n",
    "    poly = PolynomialFeatures(2)\n",
    "\n",
    "    # Randomly select 1000 extra polynomial features\n",
    "    num_extra_features = 1000\n",
    "    poly.fit(X_train.iloc[:, :163])\n",
    "    random_extra_features = np.random.choice(range(163, poly.n_output_features_), num_extra_features, replace=False)\n",
    "    \n",
    "    # Augment each of the corresponding feature sets\n",
    "    X_train_extra_features = pd.DataFrame(np.concatenate((X_train.values, poly.transform(X_train.iloc[:, :163])[:, random_extra_features]), axis=1))\n",
    "    X_val_extra_features = pd.DataFrame(np.concatenate((X_val.values,  poly.transform(X_val.iloc[:, :163])[:, random_extra_features]), axis=1))\n",
    "    X_test_extra_features = pd.DataFrame(np.concatenate((X_test.values, poly.transform(X_test.iloc[:, :163])[:, random_extra_features]), axis=1))\n",
    "    \n",
    "    columns_no_embed = X_train_extra_features.columns\n",
    "\n",
    "    # for catboost, add embeddings\n",
    "    X_train_extra_features['emb'] = list(X_train.iloc[:, 163:].values)\n",
    "    X_val_extra_features['emb'] = list(X_val.iloc[:, 163:].values)\n",
    "    X_test_extra_features['emb'] = list(X_test.iloc[:, 163:].values)\n",
    "    \n",
    "    # For the other models, need to normalize\n",
    "    X_pipeline = Pipeline([('scaler', RobustScaler())])\n",
    "    Y_pipeline = Pipeline([('scaler', StandardScaler())])\n",
    "\n",
    "    X_train[X_train.columns] = X_pipeline.fit_transform(X_train)\n",
    "    Y_train[Y_train.columns] = Y_pipeline.fit_transform(Y_train)\n",
    "\n",
    "    X_val[X_val.columns] = X_pipeline.transform(X_val)\n",
    "    Y_val[Y_val.columns] = Y_pipeline.transform(Y_val)\n",
    "\n",
    "    X_test[X_test.columns] = X_pipeline.transform(X_test)\n",
    "    \n",
    "    best_xgb = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"n_estimators\": 1000 if not dry_run else 1,\n",
    "        \"learning_rate\": 0.029604246449770312, \n",
    "        \"max_depth\": 8, \n",
    "        \"subsample\": 0.8080014405993786, \n",
    "        \"colsample_bytree\": 0.6684075982840267, \n",
    "        \"min_child_weight\": 20\n",
    "    }\n",
    "    xgb = MultiOutputRegressor(\n",
    "        xgboost.XGBRegressor(\n",
    "            **best_xgb\n",
    "        ),\n",
    "        n_jobs=3 if not dry_run else 1\n",
    "    )\n",
    "    print(\"Xgb: \", \n",
    "          xgb.fit(\n",
    "              X_train_extra_features[columns_no_embed], \n",
    "              Y_train, verbose=False\n",
    "            ).score(X_val_extra_features[columns_no_embed], Y_val))\n",
    "    \n",
    "    best_cat = {'learning_rate': 0.05, 'depth': 9}\n",
    "    cat = PlantTraitRegressor(\n",
    "        catboost.CatBoostRegressor(\n",
    "            iterations=2000 if not dry_run else 1,\n",
    "            embedding_features=[\"emb\"],\n",
    "            eval_metric=\"R2\",\n",
    "            early_stopping_rounds=1000,\n",
    "            use_best_model=True,\n",
    "            verbose=False,\n",
    "            **best_cat\n",
    "        ),\n",
    "        n_jobs=2 if not dry_run else 1\n",
    "    )\n",
    "    print(\"Cat: \", cat.fit(X_train_extra_features, Y_train, eval_set=(X_val_extra_features, Y_val)).score(X_val_extra_features, Y_val))\n",
    "    \n",
    "    best_lgb = {\n",
    "        \"objective\": \"regression\",\n",
    "        \"metric\": \"rmse\",\n",
    "        \"n_estimators\": 1500 if not dry_run else 1,\n",
    "        \"bagging_freq\": 1,\n",
    "        \"learning_rate\": 0.010144890360462996,\n",
    "        \"num_leaves\": 724,\n",
    "        \"subsample\": 0.9896282659716074,\n",
    "        \"colsample_bytree\": 0.2884524600576782,\n",
    "        \"min_data_in_leaf\": 61,\n",
    "        \"verbosity\": -1,\n",
    "    }\n",
    "    lgb = PlantTraitRegressor(\n",
    "        lightgbm.LGBMRegressor(\n",
    "            linear_tree = True,\n",
    "            **best_lgb\n",
    "        ),\n",
    "        n_jobs=2 if not dry_run else 1\n",
    "    )\n",
    "    print(\"Lgb: \", lgb.fit(\n",
    "        X_train_extra_features[columns_no_embed], Y_train, \n",
    "        eval_set=(X_val_extra_features[columns_no_embed], Y_val), \n",
    "        callbacks=[lightgbm.early_stopping(stopping_rounds=100)]\n",
    "    ).score(X_val_extra_features[columns_no_embed], Y_val))\n",
    "    \n",
    "    ridge = Ridge()\n",
    "    print(\"Ridge: \", ridge.fit(X_train, Y_train).score(X_val, Y_val))\n",
    "\n",
    "    reg = KNeighborsRegressor(\n",
    "        n_neighbors=7 if not dry_run else 1, \n",
    "        metric=\"manhattan\", \n",
    "        weights='distance'\n",
    "    )\n",
    "    print(\"Reg: \", reg.fit(X_train, Y_train).score(X_val, Y_val))\n",
    "    \n",
    "    mlp = train_mlp_model(\n",
    "        X_train=X_train,\n",
    "        Y_train=Y_train,\n",
    "        X_val=X_val,\n",
    "        Y_val=Y_val,\n",
    "        batch_size=256,\n",
    "        lr=5e-4,\n",
    "        dry_run=dry_run,\n",
    "        run_num=i,\n",
    "    )\n",
    "    print (\"MLP: \", mlp.score(X_val, Y_val))\n",
    "    \n",
    "    l_train_X = np.column_stack((\n",
    "        Y_pipeline.transform(xgb.predict(X_val_extra_features[columns_no_embed])), \n",
    "        Y_pipeline.transform(cat.predict(X_val_extra_features)), \n",
    "        Y_pipeline.transform(lgb.predict(X_val_extra_features[columns_no_embed])), \n",
    "        ridge.predict(X_val), \n",
    "        reg.predict(X_val), \n",
    "        mlp.predict(X_val))\n",
    "    )\n",
    "    \n",
    "    meta = Lasso(alpha=0.00006)\n",
    "    print (f\"Done: {i} with score\", meta.fit(l_train_X, Y_val).score(l_train_X, Y_val))\n",
    "\n",
    "    l_test_X = np.column_stack((\n",
    "        Y_pipeline.transform(xgb.predict(X_test_extra_features[columns_no_embed])), \n",
    "        Y_pipeline.transform(cat.predict(X_test_extra_features)), \n",
    "        Y_pipeline.transform(lgb.predict(X_test_extra_features[columns_no_embed])), \n",
    "        ridge.predict(X_test), \n",
    "        reg.predict(X_test), \n",
    "        mlp.predict(X_test))\n",
    "    )\n",
    "\n",
    "    if dry_run:\n",
    "        preds = Y_pipeline.inverse_transform(meta.predict(l_test_X).reshape(-1, 1))\n",
    "    else:\n",
    "        preds = Y_pipeline.inverse_transform(meta.predict(l_test_X))\n",
    "    preds_test += preds / kf.get_n_splits()\n",
    "    \n",
    "    print (\"================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(Y_pipeline.inverse_transform(xgb.predict(X_test_extra_features[columns_no_embed])), preds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('submission.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['id', 'X4', 'X11', 'X18', 'X26', 'X50', 'X3112'])\n",
    "    for i in range(len(preds_test)):\n",
    "      writer.writerow([test_data.plant.ids[i], preds_test[i][0], preds_test[i][1], preds_test[i][2], preds_test[i][3], preds_test[i][4], preds_test[i][5]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs480",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
